{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import yaml\n",
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = \"SSD1/lite-model_ssd_mobilenet_v1_1_metadata_2.tflite\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "interpreter = tf.lite.Interpreter(model_path=tf_model)\n",
    "interpreter.allocate_tensors()\n",
    "# Get the model inputs\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'normalized_input_image_tensor',\n",
       "  'index': 175,\n",
       "  'shape': array([  1, 300, 300,   3], dtype=int32),\n",
       "  'shape_signature': array([  1, 300, 300,   3], dtype=int32),\n",
       "  'dtype': numpy.uint8,\n",
       "  'quantization': (0.0078125, 128),\n",
       "  'quantization_parameters': {'scales': array([0.0078125], dtype=float32),\n",
       "   'zero_points': array([128], dtype=int32),\n",
       "   'quantized_dimension': 0},\n",
       "  'sparsity_parameters': {}}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'name': 'TFLite_Detection_PostProcess', 'index': 167, 'shape': array([ 1, 10,  4], dtype=int32), 'shape_signature': array([ 1, 10,  4], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'TFLite_Detection_PostProcess:1', 'index': 168, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'TFLite_Detection_PostProcess:2', 'index': 169, 'shape': array([ 1, 10], dtype=int32), 'shape_signature': array([ 1, 10], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
      "{'name': 'TFLite_Detection_PostProcess:3', 'index': 170, 'shape': array([1], dtype=int32), 'shape_signature': array([1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n"
     ]
    }
   ],
   "source": [
    "for i in output_details:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess(path):\n",
    "    \"\"\"\n",
    "    Preprocesses the input image before performing inference.\n",
    "\n",
    "    Returns:\n",
    "        image_data: Preprocessed image data ready for inference.\n",
    "    \"\"\"\n",
    "    input_width = 300\n",
    "    input_height = 300\n",
    "    # Read the input image using OpenCV\n",
    "    img = cv2.imread(path)\n",
    "\n",
    "    # Get the height and width of the input image\n",
    "    img_height, img_width = img.shape[:2]\n",
    "\n",
    "    # Convert the image color space from BGR to RGB\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Resize the image to match the input shape\n",
    "    img = cv2.resize(img, (input_width, input_height))\n",
    "\n",
    "    # Normalize the image data by dividing it by 255.0\n",
    "    image_data = np.array(img) / 255.0\n",
    "\n",
    "    # Transpose the image to have the channel dimension as the first dimension\n",
    "    # image_data = np.transpose(image_data, (2, 0, 1))  # Channel first\n",
    "\n",
    "    # Expand the dimensions of the image data to match the expected input shape\n",
    "    image_data = np.expand_dims(image_data, axis=0).astype(np.uint8)\n",
    "\n",
    "    # Return the preprocessed image data\n",
    "    return image_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_data = preprocess(\"car_person.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter.set_tensor(input_details[0]['index'], img_data )\n",
    "interpreter.invoke()\n",
    "box_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "class_data = interpreter.get_tensor(output_details[1]['index'])\n",
    "score_data = interpreter.get_tensor(output_details[2]['index'])\n",
    "N_data = interpreter.get_tensor(output_details[3]['index'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 2.8988817e-01,  1.5049994e-02,  9.8729146e-01,  7.9404360e-01],\n",
       "        [ 3.3722758e-02,  1.1624396e-03,  9.9431860e-01,  9.9260890e-01],\n",
       "        [ 3.8548976e-01,  1.3405573e-01,  9.9042028e-01,  9.9054027e-01],\n",
       "        [ 6.1274660e-01,  2.6420742e-02,  9.8115492e-01,  9.8509765e-01],\n",
       "        [ 6.5799445e-02, -7.0050359e-04,  9.6387386e-01,  5.8540654e-01],\n",
       "        [ 4.2022616e-02, -1.0624081e-03,  9.9275041e-01,  2.5891066e-01],\n",
       "        [-1.2793273e-02, -1.6335785e-02,  2.4310306e-01,  9.9643987e-01],\n",
       "        [ 3.8660854e-02,  5.9224522e-01,  9.6959364e-01,  1.0291947e+00],\n",
       "        [ 2.8355390e-02,  7.3278981e-01,  9.7908318e-01,  9.9690455e-01],\n",
       "        [ 9.1650784e-03,  2.8540042e-01,  6.7427146e-01,  9.8280370e-01]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "box_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99609375"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(score_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10.], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TFinference:\n",
    "    \"\"\"YOLOv8 object detection model class for handling inference and visualization.\"\"\"\n",
    "\n",
    "    def __init__(self, tf_model, input_image, confidence_thres, iou_thres,cocolabel_path,positive_list):\n",
    "        \"\"\"\n",
    "        Initializes an instance of the YOLOv8 class.\n",
    "\n",
    "        Args:\n",
    "            onnx_model: Path to the ONNX model.\n",
    "            input_image: Path to the input image.\n",
    "            confidence_thres: Confidence threshold for filtering detections.\n",
    "            iou_thres: IoU (Intersection over Union) threshold for non-maximum suppression.\n",
    "        \"\"\"\n",
    "        \n",
    "        self.tf_model = tf_model\n",
    "        self.input_image = input_image\n",
    "        self.confidence_thres = confidence_thres\n",
    "        self.iou_thres = iou_thres\n",
    "        self.cocolabel_path = cocolabel_path\n",
    "\n",
    "        # Load the class names from the COCO dataset\n",
    "        self.classes = self.yaml_load()['names']\n",
    "        \n",
    "        # Generate a color palette for the classes\n",
    "        self.color_palette = np.random.uniform(0, 255, size=(len(self.classes), 3))\n",
    "        self.positive_list = positive_list\n",
    "\n",
    "    def draw_detections(self, img, box, score, class_id):\n",
    "        \"\"\"\n",
    "        Draws bounding boxes and labels on the input image based on the detected objects.\n",
    "\n",
    "        Args:\n",
    "            img: The input image to draw detections on.\n",
    "            box: Detected bounding box.\n",
    "            score: Corresponding detection score.\n",
    "            class_id: Class ID for the detected object.\n",
    "\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract the coordinates of the bounding box\n",
    "        x1, y1, w, h = box\n",
    "\n",
    "        # Retrieve the color for the class ID\n",
    "        color = self.color_palette[class_id]\n",
    "\n",
    "        # Draw the bounding box on the image\n",
    "        cv2.rectangle(img, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)\n",
    "\n",
    "        # Create the label text with class name and score\n",
    "        label = f'{self.classes[class_id]}: {score:.2f}'\n",
    "\n",
    "        # Calculate the dimensions of the label text\n",
    "        (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)\n",
    "\n",
    "        # Calculate the position of the label text\n",
    "        label_x = x1\n",
    "        label_y = y1 - 10 if y1 - 10 > label_height else y1 + 10\n",
    "\n",
    "        # Draw a filled rectangle as the background for the label text\n",
    "        cv2.rectangle(img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height), color,\n",
    "                    cv2.FILLED)\n",
    "\n",
    "        # Draw the label text on the image\n",
    "        cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    def yaml_load(self):\n",
    "        with open(self.cocolabel_path, 'r') as file:\n",
    "            cfg = yaml.safe_load(file)\n",
    "        return cfg\n",
    "    \n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Preprocesses the input image before performing inference.\n",
    "\n",
    "        Returns:\n",
    "            image_data: Preprocessed image data ready for inference.\n",
    "        \"\"\"\n",
    "        # Read the input image using OpenCV\n",
    "        self.img = cv2.imread(self.input_image)\n",
    "\n",
    "        # Get the height and width of the input image\n",
    "        self.img_height, self.img_width = self.img.shape[:2]\n",
    "\n",
    "        # Convert the image color space from BGR to RGB\n",
    "        img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Resize the image to match the input shape\n",
    "        img = cv2.resize(img, (self.input_width, self.input_height))\n",
    "\n",
    "        # Normalize the image data by dividing it by 255.0\n",
    "        image_data = np.array(img) / 255.0\n",
    "\n",
    "        # Transpose the image to have the channel dimension as the first dimension\n",
    "        # image_data = np.transpose(image_data, (2, 0, 1))  # Channel first\n",
    "\n",
    "        # Expand the dimensions of the image data to match the expected input shape\n",
    "        image_data = np.expand_dims(image_data, axis=0).astype(np.float32)\n",
    "\n",
    "        # Return the preprocessed image data\n",
    "        return image_data\n",
    "\n",
    "    \n",
    "    def postprocess(self, input_image, output):\n",
    "        \"\"\"\n",
    "        Performs post-processing on the model's output to extract bounding boxes, scores, and class IDs.\n",
    "        Args:\n",
    "            input_image (numpy.ndarray): The input image.\n",
    "            output (numpy.ndarray): The output of the model.\n",
    "\n",
    "        Returns:\n",
    "            numpy.ndarray: The input image with detections drawn on it.\n",
    "        \"\"\"\n",
    "\n",
    "        # Transpose and squeeze the output to match the expected shape\n",
    "        outputs = np.transpose(np.squeeze(output[0]))\n",
    "\n",
    "        # Get the number of rows in the outputs array\n",
    "        rows = outputs.shape[0]\n",
    "\n",
    "        # Lists to store the bounding boxes, scores, and class IDs of the detections\n",
    "        boxes = []\n",
    "        scores = []\n",
    "        class_ids = []\n",
    "    \n",
    "        # Calculate the scaling factors for the bounding box coordinates\n",
    "        x_factor = self.img_width / self.input_width\n",
    "        y_factor = self.img_height / self.input_height\n",
    "\n",
    "        # Iterate over each row in the outputs array\n",
    "        for i in range(rows):\n",
    "            # Extract the class scores from the current row\n",
    "            classes_scores = outputs[i][4:]\n",
    "\n",
    "            # Find the maximum score among the class scores\n",
    "            max_score = np.amax(classes_scores)\n",
    "\n",
    "            # If the maximum score is above the confidence threshold\n",
    "            if max_score >= self.confidence_thres:\n",
    "                # Get the class ID with the highest score\n",
    "                class_id = np.argmax(classes_scores)\n",
    "\n",
    "                # Extract the bounding box coordinates from the current row\n",
    "                x, y, w, h = outputs[i][0], outputs[i][1], outputs[i][2], outputs[i][3]\n",
    "\n",
    "                # Calculate the scaled coordinates of the bounding box\n",
    "                left = int((x - w / 2) * x_factor)\n",
    "                top = int((y - h / 2) * y_factor)\n",
    "                width = int(w * x_factor)\n",
    "                height = int(h * y_factor)\n",
    "\n",
    "                # Add the class ID, score, and box coordinates to the respective lists\n",
    "                # and filter indices before NMS\n",
    "                if class_id in self.positive_list:\n",
    "                    class_ids.append(class_id)\n",
    "                    scores.append(max_score)\n",
    "                    boxes.append([left, top, width, height])\n",
    "\n",
    "        # Apply non-maximum suppression to filter out overlapping bounding boxes\n",
    "        indices = cv2.dnn.NMSBoxes(boxes, scores, self.confidence_thres, self.iou_thres)\n",
    "\n",
    "        self.predresult = []\n",
    "        # Iterate over the selected indices after non-maximum suppression\n",
    "        # for i in indices:\n",
    "        #     # Get the box, score, and class ID corresponding to the index\n",
    "        #     box = boxes[i]\n",
    "        #     score = scores[i]\n",
    "        #     class_id = class_ids[i]\n",
    "        #     self.predresult.append((class_id,box,score))\n",
    "        #     # Draw the detection on the input image\n",
    "        #     self.draw_detections(input_image, box, score, class_id)\n",
    "\n",
    "        for i in range(len(boxes)):\n",
    "            # Get the box, score, and class ID corresponding to the index\n",
    "            box = boxes[i]\n",
    "            score = scores[i]\n",
    "            class_id = class_ids[i]\n",
    "            self.predresult.append((class_id,box,score))\n",
    "            # Draw the detection on the input image\n",
    "            self.draw_detections(input_image, box, score, class_id)\n",
    "\n",
    "        # Return the modified input image\n",
    "        return input_image\n",
    "\n",
    "    def pred_result(self):\n",
    "        return self.predresult\n",
    "    \n",
    "    def main(self):\n",
    "        \"\"\"\n",
    "        Performs inference using an ONNX model and returns the output image with drawn detections.\n",
    "\n",
    "        Returns:\n",
    "            output_img: The output image with drawn detections.\n",
    "        \"\"\"\n",
    "        # Create an inference session using the ONNX model and specify execution providers\n",
    "        # session = ort.InferenceSession(self.onnx_model, providers=['CUDAExecutionProvider', 'CPUExecutionProvider'])\n",
    "        interpreter = tf.lite.Interpreter(model_path=self.tf_model)\n",
    "        interpreter.allocate_tensors()\n",
    "        # Get the model inputs\n",
    "        input_details = interpreter.get_input_details()\n",
    "        output_details = interpreter.get_output_details()\n",
    "        # print(output_details)\n",
    "\n",
    "        # Store the shape of the input for later use\n",
    "        input_shape = input_details[0]['shape']\n",
    "        self.input_width = input_shape[1]\n",
    "        self.input_height = input_shape[2]\n",
    "\n",
    "        # Preprocess the image data\n",
    "        img_data = self.preprocess()\n",
    "\n",
    "        # Run inference using the preprocessed image data\n",
    "        interpreter.set_tensor(input_details[0]['index'], img_data )\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "        # print(output_data)\n",
    "        # Perform post-processing on the outputs to obtain output image.\n",
    "        return self.postprocess(self.img,output_data )  # output image\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nxp",
   "language": "python",
   "name": "nxp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
